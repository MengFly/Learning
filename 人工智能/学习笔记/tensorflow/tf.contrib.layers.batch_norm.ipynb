{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.contrib.layers.batch_norm\n",
    "> 批量标准化，通过减少内部协变量转换加速深度网络训练\n",
    "> 它可以作为卷积神经网络和全连接层的标准化算法。\n",
    "\n",
    "> 如果数据类型（data_type）是NCWH即（batch,channels,weight,height）.那么会将除第二个维度外的数据全部标准化\n",
    "\n",
    "> 如果数据类型（data_type）是NWHC即（batch,weight,height,channels）,那么会将除最后一个维度外数据全部标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用batch_norm可以缩小绝对差异，增加相对差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tf.contrib.layers.batch_norm(\n",
    "    inputs,\n",
    "    decay=0.999,\n",
    "    center=True,\n",
    "    scale=False,\n",
    "    epsilon=0.001,\n",
    "    activation_fn=None,\n",
    "    param_initializers=None,\n",
    "    param_regularizers=None,\n",
    "    updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "    is_training=True,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    batch_weights=None,\n",
    "    fused=None,\n",
    "    data_format=DATA_FORMAT_NHWC,\n",
    "    zero_debias_moving_mean=False,\n",
    "    scope=None,\n",
    "    renorm=False,\n",
    "    renorm_clipping=None,\n",
    "    renorm_decay=0.99,\n",
    "    adjustment=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练的时候，moving_mean 和moving_variance 需要被更新。可以通过两种方式进行设置。\n",
    "\n",
    "1. 设置 *update_collections=None*,但是会损失速度\n",
    "2. 通过添加依赖\n",
    "  ```python\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数\n",
    "\n",
    "+ **inputs**  \n",
    "  输入，维度大于等于2，第一个维度代表Batch_size,因为是批量标准化，因此在这里第一个维度必须是Batch_size, 也因为如此，维度才需要大于等于2.\n",
    "+ *deacy=0.99* (感觉采取默认值就可以)  \n",
    "  移动平均线的衰减。衰减的合理值接近1.0，通常在多9的范围内:0.999、0.99、0.9等。如果模型经历了相当好的训练性能，但验证和/或测试性能较差，则衰减值较低(建议尝试衰减=0.9)。为了提高稳定性，请尝试zero_debias_moving_mean=True\n",
    "+ center  \n",
    "  中心:如果为真，将偏移量加到归一化张量中。如果为假，则忽略。\n",
    "+ scale  \n",
    "  比例:如果是真的，乘以。如果为假，则不使用gamma。当下一层是线性的(也如nn.relu)，这可以禁用，因为缩放可以由下一层完成。\n",
    "+ **epsilon=1e-5**  \n",
    "  向方差中加入小浮点数，以避免除以0。\n",
    "+ activation_fn  \n",
    "  激活函数（感觉一般来说都会在标准化层后面添加一层非线性层的，所以这个参数指定不指定都可以吧）\n",
    "+ param_initializers  \n",
    "  为模型的参数指定可选的初始化函数，感觉不需要，如果我们这些参数都是我们自己指定的话\n",
    "+ param_regularizers  \n",
    "  参数的正则化其，感觉也不需要\n",
    "+ update_collections=None  \n",
    "  这个在上面说过了，在训练的时候要更新参数，因此这个需要指定，当然设置成None的话更为方便。\n",
    "+ **is_training**  \n",
    "  是否是训练过程，很显然，在这个方法内部使用了moment滑动平均算法，因此在训练的时候会根据我们上面设置的deacy参数更新权重，而如果不是训练的话，就不需要更新了，只需要用之前训练的时候的权重参数即可，因此这个参数必不可少。\n",
    "+ reuse  \n",
    "  指示该层的参数是否可以被重用，如果被冲用的话，需要指定scope\n",
    "+ variables_collections\n",
    "+ outputs_collections\n",
    "+ trainable  \n",
    "  表示该层是否是可训练的\n",
    "+ batch_weight  \n",
    "  权重\n",
    "+ fused  \n",
    "  默认即可\n",
    "+ dataformat  \n",
    "  两种方式 \"NWHC\"(默认) 和 \"NCWH\"\n",
    "+ renorm  \n",
    "+ renorm_clipping\n",
    "+ renorm_decay\n",
    "+ adjustment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
