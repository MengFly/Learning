{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tensorflow开源的Cifar10来对数据进行读取\n",
    "这里，我把它的代码搬到了我的笔记的同级目录下面了，下载google的代码仓库可以执行下面的命令\n",
    "```shell\n",
    "    git clone https://github.com/tensorflow/modles.git\n",
    "```\n",
    "然后进入cifar10这个项目的目录中就可以看见它的源代码了。\n",
    "```shell\n",
    "    cd models/tutorials/image/cifar10\n",
    "```\n",
    "\n",
    "下面是这个神经网络的流程结构图：\n",
    "\n",
    "|Layer名称|描述|\n",
    "|---------|----|\n",
    "|conv1|卷积层和ReLU激活函数|\n",
    "|pool1|最大池化|\n",
    "|norm1|LRN|\n",
    "|conv2|卷积层和ReLU激活函数|\n",
    "|norm2|LRN|\n",
    "|pool2|最大池化|\n",
    "|fla1|全连接层和ReLU激活函数|\n",
    "|fla2|全连接层和ReLU激活函数|\n",
    "|logits|模型Inference输出结果|\n",
    "+ **input_image**\n",
    " - 数据增强\n",
    " \n",
    "+ **conv1**\n",
    " - 卷积层（权重不使用L2正则化）卷积核[5,5,3,64]，步长[1,1,1,1],Padding=\"SAME\"\n",
    " - 激活函数ReLU\n",
    " - 最大池化层ksize[1,3,3,1], 步长[1,2,2,1], Padding=\"SAME\"\n",
    " - LRN算法\n",
    "\n",
    "+ **conv2**\n",
    " - 卷积层（权重不使用L2正则化）卷积核[5,5,64,64]，步长[1,1,1,1],Padding=\"SAME\"\n",
    " - 激活函数ReLU\n",
    " - LRN算法\n",
    " - 最大池化层ksize[1,3,3,1], 步长[1,2,2,1], Padding=\"SAME\"\n",
    " \n",
    "+ **fla1**\n",
    " - 权重使用L2正则化\n",
    " - 节点数 384\n",
    " - reshape:[batch_size, -1]\n",
    " - 激活函数ReLU\n",
    "\n",
    "+ **fla2**\n",
    " - 权重使用L2正则化\n",
    " - 节点数 192\n",
    " - 激活函数ReLU\n",
    "\n",
    "+ **fla3**\n",
    " - 权重不使用L2正则化\n",
    " - 节点数 10\n",
    " - 不使用激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10, cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我手动下载了cifar-10的数据，并把它解压在了我的D盘的download文件夹下面的cifar-10-data目录下面了，因此我这里写的data_dir = \"D:\\download\\cifar-10-data\\cifar-10-batches-bin\",数据的下载地址为：[http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz](http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz)\n",
    "\n",
    "\n",
    "这里设置总共迭代数据3000次\n",
    "\n",
    "每一个mini-batch-size为128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step = 3000\n",
    "batch_size = 128\n",
    "data_dir = \"D:\\download\\cifar-10-data\\cifar-10-batches-bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义初始化权重的函数，和之前不同的是，这里 给weight添加了一个L2的loss。相当于做了一个L2的正则化处理。\n",
    "为了理解L2正则化我们先把L2正则化的公式列下来：\n",
    "$$ J(w,b) = \\frac{1}{m}\\sum^{m}_{i=1}L(w,b) + \\frac{\\gamma}{2m}||w||^{2}_{2}$$\n",
    "其实也就是给成本函数添加一个正则项。\n",
    "这样一来，调整参数的时候，公式就变成了这样：\n",
    "$$ w = (1-\\alpha\\frac{\\gamma}{m})w - \\alpha\\frac{\\partial J}{\\partial w}$$\n",
    "对正则化的理解：首先，正则化是为了减少模型的过拟合，一般我们可以通过减少特征或者惩罚特征来达到这一个目的，因为通过惩罚特征，不让神经网络过度依赖某一些特征可以提高神经网络的泛化性，而正则化就是帮助我们来惩罚特征的。在上面的第一个式子我们可以 看到L2正则化就是在损失函数后面添加了一个权重损失项，也可以这么理解，为了使用某个权重，我们就需要付出某些loss的代价，除非这个特征非常有效，否则就会被loss上的增加覆盖效果。这样我们就可以筛选出最有效的特征，减少特征权重过拟合。\n",
    "\n",
    "因此通过对上面公式的理解，我们有了下面的代码，我们首先，先计算出每一个权重的l2正则化的损失\n",
    "```python\n",
    "    tf.nn.l2_loss(var)\n",
    "```\n",
    "同时，为了我们可以控制L2正则化的程度，我们又给得到的L2正则化损失添加了一个权重，即下面的代码\n",
    "```python\n",
    "    tf.multiply(tf.nn.l2_loss(var), wl, name = 'weight_loss')\n",
    "```\n",
    "这就是最终的weight_loss,而通过上面的公式我们可以看到，最终的loss是要添加到损失函数上面的，因此我们这里先把我们的weight_loss先保存起来，到计算损失函数的时候再进行使用，下面的代码就是保存我们weight_loss的代码\n",
    "```python \n",
    "   tf.add_to_collection('losses', weight_loss)\n",
    "```\n",
    "因为我们并不是对 我们的权重做的操作，因此只需要原班不动的返回我们的权重即可\n",
    "```python\n",
    "    return var\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_with_weight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name = 'weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用distorted_inputs产生训练数据，这里返回是封装好的tensor，每次执行都会生成一个batch_size数量的样本，同时，这里使用了Data Augmentation（数据增强），数据增强包括随机的顺平翻转，随机剪切一块24\\*24大小的图片，设置随机的亮度和对比度，以及对数据进行标准化。\n",
    "\n",
    "另外，distorted_inputs使用了16个独立的线程来加速任务，函数内部会产生线程池，在需要使用时会通过TensorFlow queue进行调度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "images_train, labels_train = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用cifar10_inputs.inputs函数生成测试数据，这里不需要对图片进行翻转或修改亮度，对比度，不过需要裁剪图片正中间的24x24大小的区块，并对数据进行标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test, labels_test = cifar10_input.inputs(eval_data=True, data_dir = data_dir, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里定义两个占位符，不再多说，一个是训练集X，一个是输出标签Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\n",
    "label_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义第一个卷积层，这个卷积层中\n",
    "\n",
    "首先我们定义权重参数W（weight1）--卷积核\n",
    "\n",
    "然后让卷积核和输入进行卷积操作\n",
    "\n",
    "然后应用激活函数，这里使用的是ReLU函数，这里的bias_add是add的一种特例，使用情况是必须保证bias和input的最后一维的维度相同，这里，我们的卷积处理完成之后，数据的维度是[batch_size, w, h, 64]，我们的bias维度是[64]，因此，维度相同\n",
    "\n",
    "之后使用一个尺寸为3\\*3，步长为2\\*2的最大池化层处理数据。这里的最大池化的尺寸和步长不一致，这样可以增加数据的丰富性。\n",
    "\n",
    "最后使用**tf.nn.lrn**函数，即LRN对结果进行处理，Alex在论文中解释LRN层模仿了生物神经系统的侧抑制机制，对局部神经元的活动创建竞争环境，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。Alex在ImageNet数据集上的实验表明，使用LRN后CNN在Top1的错误率可以降低1.4%，因此在其经典的AlexNet中使用了LRN层。LRN对ReLU这种没有上限边界的激活函数会比较有用，因为它会从附近的多个卷积核的相应中挑选比较大的反馈，但不适合sigmoid这种有固定边界并且能抑制过大值的激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = variable_with_weight_loss(shape=[5,5,3,64], stddev=5e-2, wl=0.0)\n",
    "kernel = tf.nn.conv2d(image_holder, weight1, [1,1,1,1], padding=\"SAME\")\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel, bias1))\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias = 1.0, alpha=0.001/9.0, beta=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建第二个卷积层，这里的初始化参数以及卷积操作就不多说了。\n",
    "\n",
    "这里第二个卷积层和上面的第一个卷积层的不同的地方是，这里我们**调换了最大池化层和LRN层的顺序**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight2 = variable_with_weight_loss(shape=[5,5,64,64], stddev=5e-2, wl=0.0)\n",
    "kernel2 = tf.nn.conv2d(norm1, weight2, strides=[1,1,1,1], padding='SAME')\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias = 1.0, alpha = 0.001/9.0, beta = 0.75)\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来使用全连接层，同样的模板操作，将上一层的输出reshape成一维数据。\n",
    "这里有一个技巧，我们不需要再进行计算最终的卷积层输出是什么了，而是直接使用[batch_size, -1]来进行reshape，这样就减少了reshape的计算维度的步骤了，之后我们可以直接使用方法**reshape.get_shape()[1].value**来获取卷积层的最终输出维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, wl = 0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个全连接层，不用赘述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三个全连接层，需要注意的是，这里不像之前那样使用softmax输出最后结果，这是因为我们把softmax的操作放在了计算loss的部分，我们不需要对inference的输出进行softmax处理就可以获得最终分类结果。（直接比较inference输出的各类的数值大小即可），计算softmax只要是为了计算loss，因此，softmax操作整合到后面是比较合适的。\n",
    "\n",
    "**这里说明一下，首先，无论是否使用softmax激活函数，结果是输出哪一个类别并不会改变，softmax的作用就是把输出结果归一化，并不会影响各个输出值的大小，这是作者不使用softmax的原因。另外，之所以使用softmax是因为后面计算损失函数的方便，因此，作者把softmax的计算归到了loss里面。因为我们在之前学习多类别训练的时候使用的就是softmax函数，损失函数也是在这个基础上计算出来的，其实无论最后一层是否使用softmax函数，不影响最终的输出，但是loss函数的计算得运用softmax函数。只不过如果最后一层不适用softmax函数的话，那么，每一个节点的输出的最终结果也就不会是概率了。总之，这也是换了一种方式的实现吧。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight5 = variable_with_weight_loss(shape=[192, 10], stddev=1/192.0, wl = 0.)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "logits = tf.add(tf.matmul(local4, weight5), bias5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算网络的损失函数，这里我们把softmax和cross_entropy_loss的计算合在了一起，即函数**tf.nn.sparse_softmax_cross_entropy_with_logits**, *也就是说，如果在多分类网络模型中，如果最后一层的输出我们不使用softmax激活函数二把softmax归并到损失函数里面进行计算的话，可以直接使用上面的函数。*\n",
    "\n",
    "在计算出没有使用正则化的损失函数的值之后，我们把这个损失值也添加到整体的loss的collection中，最后使用tf.add_n将整体losses的collection中的全部loss进行求和，这样也就得到了含有L2正则项的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                  labels=labels,\n",
    "                                                                  name = 'cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\"cross_entropy\")\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将模型的输出logits和数据的标签来计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss(logits, label_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置优化器，如果max_steps比较大，那么推荐使用学习速率衰减的SGD进行训练，这样训练过程中能达到的准确率峰值会比较高，大致接近86%,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tf.nn.in_top_k函数求输出结果中top_k的准确率，默认使用top_1,也就是输出分数最高的那一类的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启动前面提到的图片数据增强的线程队列，这里一共使用了16个线程来进行训练加速，如果这里不启动线程，那么后续的inference及训练的操作都是无法开始的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-6, started daemon 14052)>,\n",
       " <Thread(Thread-7, started daemon 17100)>,\n",
       " <Thread(Thread-8, started daemon 7004)>,\n",
       " <Thread(Thread-9, started daemon 16208)>,\n",
       " <Thread(Thread-10, started daemon 3356)>,\n",
       " <Thread(Thread-11, started daemon 12008)>,\n",
       " <Thread(Thread-12, started daemon 13052)>,\n",
       " <Thread(Thread-13, started daemon 13432)>,\n",
       " <Thread(Thread-14, started daemon 10996)>,\n",
       " <Thread(Thread-15, started daemon 11484)>,\n",
       " <Thread(Thread-16, started daemon 8196)>,\n",
       " <Thread(Thread-17, started daemon 20232)>,\n",
       " <Thread(Thread-18, started daemon 12620)>,\n",
       " <Thread(Thread-19, started daemon 16748)>,\n",
       " <Thread(Thread-20, started daemon 13836)>,\n",
       " <Thread(Thread-21, started daemon 21340)>,\n",
       " <Thread(Thread-22, started daemon 5748)>,\n",
       " <Thread(Thread-23, started daemon 23304)>,\n",
       " <Thread(Thread-24, started daemon 1952)>,\n",
       " <Thread(Thread-25, started daemon 2308)>,\n",
       " <Thread(Thread-26, started daemon 19004)>,\n",
       " <Thread(Thread-27, started daemon 12188)>,\n",
       " <Thread(Thread-28, started daemon 2280)>,\n",
       " <Thread(Thread-29, started daemon 17280)>,\n",
       " <Thread(Thread-30, started daemon 21904)>,\n",
       " <Thread(Thread-31, started daemon 14560)>,\n",
       " <Thread(Thread-32, started daemon 22768)>,\n",
       " <Thread(Thread-33, started daemon 19900)>,\n",
       " <Thread(Thread-34, started daemon 8552)>,\n",
       " <Thread(Thread-35, started daemon 2368)>,\n",
       " <Thread(Thread-36, started daemon 16180)>,\n",
       " <Thread(Thread-37, started daemon 5128)>,\n",
       " <Thread(Thread-38, started daemon 23812)>,\n",
       " <Thread(Thread-39, started daemon 20460)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss=4.68 (82.5 example/sec; 1.552 sec/batch)\n",
      "step 10, loss=3.70 (113.5 example/sec; 1.127 sec/batch)\n",
      "step 20, loss=3.03 (111.4 example/sec; 1.149 sec/batch)\n",
      "step 30, loss=2.68 (107.3 example/sec; 1.193 sec/batch)\n",
      "step 40, loss=2.49 (93.7 example/sec; 1.366 sec/batch)\n",
      "step 50, loss=2.28 (119.5 example/sec; 1.071 sec/batch)\n",
      "step 60, loss=2.18 (112.7 example/sec; 1.136 sec/batch)\n",
      "step 70, loss=2.08 (111.5 example/sec; 1.148 sec/batch)\n",
      "step 80, loss=2.11 (112.0 example/sec; 1.143 sec/batch)\n",
      "step 90, loss=2.04 (107.8 example/sec; 1.187 sec/batch)\n",
      "step 100, loss=2.01 (113.4 example/sec; 1.129 sec/batch)\n",
      "step 110, loss=1.98 (102.9 example/sec; 1.244 sec/batch)\n",
      "step 120, loss=1.92 (107.2 example/sec; 1.194 sec/batch)\n",
      "step 130, loss=1.88 (111.3 example/sec; 1.150 sec/batch)\n",
      "step 140, loss=1.79 (99.8 example/sec; 1.283 sec/batch)\n",
      "step 150, loss=1.86 (108.8 example/sec; 1.177 sec/batch)\n",
      "step 160, loss=1.91 (115.5 example/sec; 1.108 sec/batch)\n",
      "step 170, loss=1.80 (113.3 example/sec; 1.130 sec/batch)\n",
      "step 180, loss=1.68 (105.0 example/sec; 1.219 sec/batch)\n",
      "step 190, loss=1.69 (100.7 example/sec; 1.271 sec/batch)\n",
      "step 200, loss=1.70 (109.6 example/sec; 1.168 sec/batch)\n",
      "step 210, loss=1.79 (104.8 example/sec; 1.221 sec/batch)\n",
      "step 220, loss=1.65 (104.5 example/sec; 1.225 sec/batch)\n",
      "step 230, loss=1.76 (114.1 example/sec; 1.122 sec/batch)\n",
      "step 240, loss=1.59 (104.5 example/sec; 1.224 sec/batch)\n",
      "step 250, loss=1.67 (100.3 example/sec; 1.276 sec/batch)\n",
      "step 260, loss=1.49 (111.2 example/sec; 1.151 sec/batch)\n",
      "step 270, loss=1.73 (109.3 example/sec; 1.171 sec/batch)\n",
      "step 280, loss=1.48 (105.8 example/sec; 1.210 sec/batch)\n",
      "step 290, loss=1.53 (97.0 example/sec; 1.319 sec/batch)\n",
      "step 300, loss=1.56 (112.3 example/sec; 1.140 sec/batch)\n",
      "step 310, loss=1.63 (114.5 example/sec; 1.118 sec/batch)\n",
      "step 320, loss=1.70 (112.3 example/sec; 1.139 sec/batch)\n",
      "step 330, loss=1.53 (112.7 example/sec; 1.135 sec/batch)\n",
      "step 340, loss=1.47 (106.0 example/sec; 1.207 sec/batch)\n",
      "step 350, loss=1.56 (112.1 example/sec; 1.141 sec/batch)\n",
      "step 360, loss=1.60 (115.2 example/sec; 1.111 sec/batch)\n",
      "step 370, loss=1.56 (114.0 example/sec; 1.123 sec/batch)\n",
      "step 380, loss=1.64 (115.9 example/sec; 1.104 sec/batch)\n",
      "step 390, loss=1.38 (95.0 example/sec; 1.347 sec/batch)\n",
      "step 400, loss=1.48 (102.1 example/sec; 1.253 sec/batch)\n",
      "step 410, loss=1.55 (110.6 example/sec; 1.157 sec/batch)\n",
      "step 420, loss=1.48 (101.6 example/sec; 1.259 sec/batch)\n",
      "step 430, loss=1.43 (115.2 example/sec; 1.111 sec/batch)\n",
      "step 440, loss=1.53 (115.7 example/sec; 1.106 sec/batch)\n",
      "step 450, loss=1.63 (111.4 example/sec; 1.149 sec/batch)\n",
      "step 460, loss=1.40 (112.7 example/sec; 1.135 sec/batch)\n",
      "step 470, loss=1.42 (111.5 example/sec; 1.148 sec/batch)\n",
      "step 480, loss=1.46 (103.4 example/sec; 1.238 sec/batch)\n",
      "step 490, loss=1.34 (94.4 example/sec; 1.356 sec/batch)\n",
      "step 500, loss=1.50 (114.1 example/sec; 1.122 sec/batch)\n",
      "step 510, loss=1.44 (112.9 example/sec; 1.134 sec/batch)\n",
      "step 520, loss=1.47 (116.2 example/sec; 1.102 sec/batch)\n",
      "step 530, loss=1.57 (110.6 example/sec; 1.157 sec/batch)\n",
      "step 540, loss=1.41 (104.0 example/sec; 1.231 sec/batch)\n",
      "step 550, loss=1.51 (104.9 example/sec; 1.220 sec/batch)\n",
      "step 560, loss=1.67 (102.4 example/sec; 1.250 sec/batch)\n",
      "step 570, loss=1.48 (103.7 example/sec; 1.235 sec/batch)\n",
      "step 580, loss=1.38 (108.6 example/sec; 1.178 sec/batch)\n",
      "step 590, loss=1.37 (111.4 example/sec; 1.149 sec/batch)\n",
      "step 600, loss=1.35 (111.2 example/sec; 1.151 sec/batch)\n",
      "step 610, loss=1.48 (109.0 example/sec; 1.174 sec/batch)\n",
      "step 620, loss=1.37 (108.2 example/sec; 1.183 sec/batch)\n",
      "step 630, loss=1.26 (107.2 example/sec; 1.194 sec/batch)\n",
      "step 640, loss=1.47 (113.3 example/sec; 1.130 sec/batch)\n",
      "step 650, loss=1.46 (116.4 example/sec; 1.100 sec/batch)\n",
      "step 660, loss=1.52 (91.1 example/sec; 1.406 sec/batch)\n",
      "step 670, loss=1.41 (101.5 example/sec; 1.261 sec/batch)\n",
      "step 680, loss=1.39 (98.4 example/sec; 1.301 sec/batch)\n",
      "step 690, loss=1.35 (105.8 example/sec; 1.210 sec/batch)\n",
      "step 700, loss=1.32 (97.5 example/sec; 1.313 sec/batch)\n",
      "step 710, loss=1.52 (95.3 example/sec; 1.342 sec/batch)\n",
      "step 720, loss=1.41 (109.8 example/sec; 1.165 sec/batch)\n",
      "step 730, loss=1.18 (111.7 example/sec; 1.146 sec/batch)\n",
      "step 740, loss=1.29 (106.9 example/sec; 1.197 sec/batch)\n",
      "step 750, loss=1.34 (107.4 example/sec; 1.192 sec/batch)\n",
      "step 760, loss=1.49 (108.0 example/sec; 1.185 sec/batch)\n",
      "step 770, loss=1.40 (105.0 example/sec; 1.219 sec/batch)\n",
      "step 780, loss=1.36 (102.8 example/sec; 1.245 sec/batch)\n",
      "step 790, loss=1.28 (106.5 example/sec; 1.201 sec/batch)\n",
      "step 800, loss=1.44 (100.1 example/sec; 1.279 sec/batch)\n",
      "step 810, loss=1.64 (112.9 example/sec; 1.134 sec/batch)\n",
      "step 820, loss=1.26 (110.9 example/sec; 1.154 sec/batch)\n",
      "step 830, loss=1.46 (103.7 example/sec; 1.234 sec/batch)\n",
      "step 840, loss=1.39 (103.8 example/sec; 1.233 sec/batch)\n",
      "step 850, loss=1.32 (102.8 example/sec; 1.245 sec/batch)\n",
      "step 860, loss=1.36 (95.8 example/sec; 1.335 sec/batch)\n",
      "step 870, loss=1.32 (104.7 example/sec; 1.222 sec/batch)\n",
      "step 880, loss=1.17 (103.0 example/sec; 1.242 sec/batch)\n",
      "step 890, loss=1.42 (108.2 example/sec; 1.183 sec/batch)\n",
      "step 900, loss=1.29 (109.6 example/sec; 1.168 sec/batch)\n",
      "step 910, loss=1.27 (108.3 example/sec; 1.182 sec/batch)\n",
      "step 920, loss=1.28 (96.8 example/sec; 1.322 sec/batch)\n",
      "step 930, loss=1.31 (114.7 example/sec; 1.116 sec/batch)\n",
      "step 940, loss=1.44 (114.7 example/sec; 1.116 sec/batch)\n",
      "step 950, loss=1.36 (113.4 example/sec; 1.128 sec/batch)\n",
      "step 960, loss=1.24 (99.7 example/sec; 1.283 sec/batch)\n",
      "step 970, loss=1.30 (114.4 example/sec; 1.119 sec/batch)\n",
      "step 980, loss=1.26 (110.3 example/sec; 1.160 sec/batch)\n",
      "step 990, loss=1.11 (100.6 example/sec; 1.273 sec/batch)\n",
      "step 1000, loss=1.17 (106.1 example/sec; 1.207 sec/batch)\n",
      "step 1010, loss=1.16 (104.4 example/sec; 1.226 sec/batch)\n",
      "step 1020, loss=1.26 (81.2 example/sec; 1.576 sec/batch)\n",
      "step 1030, loss=1.33 (101.2 example/sec; 1.265 sec/batch)\n",
      "step 1040, loss=1.16 (81.9 example/sec; 1.564 sec/batch)\n",
      "step 1050, loss=1.31 (82.2 example/sec; 1.558 sec/batch)\n",
      "step 1060, loss=1.40 (91.3 example/sec; 1.402 sec/batch)\n",
      "step 1070, loss=1.30 (82.2 example/sec; 1.557 sec/batch)\n",
      "step 1080, loss=1.32 (116.0 example/sec; 1.104 sec/batch)\n",
      "step 1090, loss=1.17 (115.4 example/sec; 1.109 sec/batch)\n",
      "step 1100, loss=1.20 (117.9 example/sec; 1.085 sec/batch)\n",
      "step 1110, loss=1.13 (112.2 example/sec; 1.141 sec/batch)\n",
      "step 1120, loss=1.41 (115.0 example/sec; 1.113 sec/batch)\n",
      "step 1130, loss=1.22 (116.1 example/sec; 1.103 sec/batch)\n",
      "step 1140, loss=1.03 (115.2 example/sec; 1.112 sec/batch)\n",
      "step 1150, loss=1.33 (117.4 example/sec; 1.090 sec/batch)\n",
      "step 1160, loss=1.17 (116.9 example/sec; 1.095 sec/batch)\n",
      "step 1170, loss=1.33 (114.8 example/sec; 1.115 sec/batch)\n",
      "step 1180, loss=1.12 (115.9 example/sec; 1.105 sec/batch)\n",
      "step 1190, loss=1.17 (115.9 example/sec; 1.104 sec/batch)\n",
      "step 1200, loss=1.46 (115.3 example/sec; 1.110 sec/batch)\n",
      "step 1210, loss=1.26 (113.5 example/sec; 1.127 sec/batch)\n",
      "step 1220, loss=1.24 (115.5 example/sec; 1.108 sec/batch)\n",
      "step 1230, loss=1.18 (115.3 example/sec; 1.110 sec/batch)\n",
      "step 1240, loss=1.18 (111.3 example/sec; 1.150 sec/batch)\n",
      "step 1250, loss=1.26 (115.9 example/sec; 1.105 sec/batch)\n",
      "step 1260, loss=1.26 (116.3 example/sec; 1.101 sec/batch)\n",
      "step 1270, loss=1.19 (114.5 example/sec; 1.118 sec/batch)\n",
      "step 1280, loss=1.15 (117.3 example/sec; 1.091 sec/batch)\n",
      "step 1290, loss=1.24 (112.9 example/sec; 1.134 sec/batch)\n",
      "step 1300, loss=1.11 (111.3 example/sec; 1.150 sec/batch)\n",
      "step 1310, loss=1.24 (115.3 example/sec; 1.110 sec/batch)\n",
      "step 1320, loss=1.13 (111.9 example/sec; 1.144 sec/batch)\n",
      "step 1330, loss=1.38 (114.2 example/sec; 1.121 sec/batch)\n",
      "step 1340, loss=1.44 (116.0 example/sec; 1.104 sec/batch)\n",
      "step 1350, loss=1.02 (115.6 example/sec; 1.107 sec/batch)\n",
      "step 1360, loss=1.18 (108.3 example/sec; 1.182 sec/batch)\n",
      "step 1370, loss=1.44 (80.5 example/sec; 1.589 sec/batch)\n",
      "step 1380, loss=1.30 (111.1 example/sec; 1.152 sec/batch)\n",
      "step 1390, loss=1.17 (93.6 example/sec; 1.367 sec/batch)\n",
      "step 1400, loss=1.18 (113.1 example/sec; 1.131 sec/batch)\n",
      "step 1410, loss=1.33 (112.2 example/sec; 1.140 sec/batch)\n",
      "step 1420, loss=1.11 (112.1 example/sec; 1.142 sec/batch)\n",
      "step 1430, loss=1.03 (114.8 example/sec; 1.115 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1440, loss=1.27 (117.4 example/sec; 1.090 sec/batch)\n",
      "step 1450, loss=1.32 (112.6 example/sec; 1.136 sec/batch)\n",
      "step 1460, loss=1.18 (107.3 example/sec; 1.193 sec/batch)\n",
      "step 1470, loss=1.29 (117.9 example/sec; 1.086 sec/batch)\n",
      "step 1480, loss=1.14 (97.8 example/sec; 1.308 sec/batch)\n",
      "step 1490, loss=1.32 (102.0 example/sec; 1.255 sec/batch)\n",
      "step 1500, loss=1.21 (113.8 example/sec; 1.124 sec/batch)\n",
      "step 1510, loss=1.16 (116.8 example/sec; 1.096 sec/batch)\n",
      "step 1520, loss=1.11 (114.7 example/sec; 1.116 sec/batch)\n",
      "step 1530, loss=1.15 (105.2 example/sec; 1.216 sec/batch)\n",
      "step 1540, loss=1.22 (114.4 example/sec; 1.119 sec/batch)\n",
      "step 1550, loss=1.23 (94.1 example/sec; 1.360 sec/batch)\n",
      "step 1560, loss=1.22 (93.8 example/sec; 1.364 sec/batch)\n",
      "step 1570, loss=1.38 (115.8 example/sec; 1.105 sec/batch)\n",
      "step 1580, loss=1.16 (107.0 example/sec; 1.196 sec/batch)\n",
      "step 1590, loss=1.21 (112.3 example/sec; 1.139 sec/batch)\n",
      "step 1600, loss=1.11 (115.3 example/sec; 1.110 sec/batch)\n",
      "step 1610, loss=1.15 (91.0 example/sec; 1.407 sec/batch)\n",
      "step 1620, loss=1.15 (108.0 example/sec; 1.185 sec/batch)\n",
      "step 1630, loss=1.21 (116.3 example/sec; 1.101 sec/batch)\n",
      "step 1640, loss=1.15 (86.6 example/sec; 1.477 sec/batch)\n",
      "step 1650, loss=1.27 (100.5 example/sec; 1.274 sec/batch)\n",
      "step 1660, loss=1.14 (80.9 example/sec; 1.583 sec/batch)\n",
      "step 1670, loss=1.35 (96.3 example/sec; 1.329 sec/batch)\n",
      "step 1680, loss=1.22 (111.0 example/sec; 1.153 sec/batch)\n",
      "step 1690, loss=1.26 (80.8 example/sec; 1.584 sec/batch)\n",
      "step 1700, loss=1.22 (117.4 example/sec; 1.090 sec/batch)\n",
      "step 1710, loss=1.19 (118.3 example/sec; 1.082 sec/batch)\n",
      "step 1720, loss=1.29 (93.2 example/sec; 1.373 sec/batch)\n",
      "step 1730, loss=1.36 (92.0 example/sec; 1.391 sec/batch)\n",
      "step 1740, loss=1.11 (115.3 example/sec; 1.110 sec/batch)\n",
      "step 1750, loss=1.18 (92.1 example/sec; 1.390 sec/batch)\n",
      "step 1760, loss=1.02 (91.7 example/sec; 1.396 sec/batch)\n",
      "step 1770, loss=1.08 (105.1 example/sec; 1.217 sec/batch)\n",
      "step 1780, loss=1.16 (116.7 example/sec; 1.097 sec/batch)\n",
      "step 1790, loss=1.19 (114.7 example/sec; 1.116 sec/batch)\n",
      "step 1800, loss=1.02 (114.5 example/sec; 1.118 sec/batch)\n",
      "step 1810, loss=1.02 (113.4 example/sec; 1.129 sec/batch)\n",
      "step 1820, loss=1.12 (116.5 example/sec; 1.099 sec/batch)\n",
      "step 1830, loss=1.02 (91.5 example/sec; 1.398 sec/batch)\n",
      "step 1840, loss=1.02 (107.9 example/sec; 1.186 sec/batch)\n",
      "step 1850, loss=0.96 (94.1 example/sec; 1.360 sec/batch)\n",
      "step 1860, loss=1.03 (92.3 example/sec; 1.388 sec/batch)\n",
      "step 1870, loss=1.16 (111.6 example/sec; 1.147 sec/batch)\n",
      "step 1880, loss=1.19 (117.6 example/sec; 1.088 sec/batch)\n",
      "step 1890, loss=1.14 (110.0 example/sec; 1.164 sec/batch)\n",
      "step 1900, loss=1.12 (108.7 example/sec; 1.178 sec/batch)\n",
      "step 1910, loss=1.02 (115.6 example/sec; 1.107 sec/batch)\n",
      "step 1920, loss=1.27 (112.2 example/sec; 1.141 sec/batch)\n",
      "step 1930, loss=1.29 (116.7 example/sec; 1.097 sec/batch)\n",
      "step 1940, loss=1.18 (109.9 example/sec; 1.165 sec/batch)\n",
      "step 1950, loss=1.28 (116.0 example/sec; 1.103 sec/batch)\n",
      "step 1960, loss=0.97 (112.7 example/sec; 1.136 sec/batch)\n",
      "step 1970, loss=1.16 (114.0 example/sec; 1.123 sec/batch)\n",
      "step 1980, loss=1.07 (114.5 example/sec; 1.118 sec/batch)\n",
      "step 1990, loss=1.19 (113.5 example/sec; 1.127 sec/batch)\n",
      "step 2000, loss=1.04 (111.6 example/sec; 1.147 sec/batch)\n",
      "step 2010, loss=1.04 (110.2 example/sec; 1.161 sec/batch)\n",
      "step 2020, loss=1.03 (115.5 example/sec; 1.108 sec/batch)\n",
      "step 2030, loss=1.05 (108.2 example/sec; 1.183 sec/batch)\n",
      "step 2040, loss=1.16 (111.5 example/sec; 1.148 sec/batch)\n",
      "step 2050, loss=1.29 (109.2 example/sec; 1.172 sec/batch)\n",
      "step 2060, loss=1.10 (112.5 example/sec; 1.137 sec/batch)\n",
      "step 2070, loss=1.19 (95.9 example/sec; 1.334 sec/batch)\n",
      "step 2080, loss=1.12 (109.4 example/sec; 1.170 sec/batch)\n",
      "step 2090, loss=1.05 (111.1 example/sec; 1.152 sec/batch)\n",
      "step 2100, loss=1.09 (105.6 example/sec; 1.212 sec/batch)\n",
      "step 2110, loss=1.02 (117.3 example/sec; 1.091 sec/batch)\n",
      "step 2120, loss=1.23 (110.3 example/sec; 1.161 sec/batch)\n",
      "step 2130, loss=1.29 (111.3 example/sec; 1.150 sec/batch)\n",
      "step 2140, loss=0.99 (95.3 example/sec; 1.342 sec/batch)\n",
      "step 2150, loss=1.16 (112.7 example/sec; 1.136 sec/batch)\n",
      "step 2160, loss=1.08 (65.3 example/sec; 1.960 sec/batch)\n",
      "step 2170, loss=1.25 (115.2 example/sec; 1.111 sec/batch)\n",
      "step 2180, loss=1.32 (86.8 example/sec; 1.475 sec/batch)\n",
      "step 2190, loss=1.25 (112.9 example/sec; 1.134 sec/batch)\n",
      "step 2200, loss=1.09 (95.9 example/sec; 1.335 sec/batch)\n",
      "step 2210, loss=1.10 (97.4 example/sec; 1.314 sec/batch)\n",
      "step 2220, loss=1.06 (114.2 example/sec; 1.121 sec/batch)\n",
      "step 2230, loss=1.23 (106.9 example/sec; 1.197 sec/batch)\n",
      "step 2240, loss=1.04 (103.5 example/sec; 1.237 sec/batch)\n",
      "step 2250, loss=1.20 (102.4 example/sec; 1.249 sec/batch)\n",
      "step 2260, loss=1.19 (93.8 example/sec; 1.364 sec/batch)\n",
      "step 2270, loss=1.17 (99.2 example/sec; 1.291 sec/batch)\n",
      "step 2280, loss=1.10 (83.2 example/sec; 1.538 sec/batch)\n",
      "step 2290, loss=1.08 (110.6 example/sec; 1.157 sec/batch)\n",
      "step 2300, loss=1.08 (104.8 example/sec; 1.221 sec/batch)\n",
      "step 2310, loss=1.29 (111.1 example/sec; 1.152 sec/batch)\n",
      "step 2320, loss=1.11 (112.4 example/sec; 1.139 sec/batch)\n",
      "step 2330, loss=1.18 (116.1 example/sec; 1.103 sec/batch)\n",
      "step 2340, loss=1.27 (111.2 example/sec; 1.151 sec/batch)\n",
      "step 2350, loss=1.07 (99.2 example/sec; 1.291 sec/batch)\n",
      "step 2360, loss=1.47 (113.9 example/sec; 1.123 sec/batch)\n",
      "step 2370, loss=1.22 (113.7 example/sec; 1.125 sec/batch)\n",
      "step 2380, loss=1.10 (103.5 example/sec; 1.236 sec/batch)\n",
      "step 2390, loss=1.18 (113.7 example/sec; 1.126 sec/batch)\n",
      "step 2400, loss=1.20 (108.8 example/sec; 1.177 sec/batch)\n",
      "step 2410, loss=1.00 (100.2 example/sec; 1.278 sec/batch)\n",
      "step 2420, loss=1.25 (104.5 example/sec; 1.225 sec/batch)\n",
      "step 2430, loss=1.10 (106.1 example/sec; 1.207 sec/batch)\n",
      "step 2440, loss=1.14 (112.7 example/sec; 1.135 sec/batch)\n",
      "step 2450, loss=1.07 (95.6 example/sec; 1.339 sec/batch)\n",
      "step 2460, loss=1.01 (115.0 example/sec; 1.113 sec/batch)\n",
      "step 2470, loss=1.08 (108.1 example/sec; 1.184 sec/batch)\n",
      "step 2480, loss=1.13 (102.2 example/sec; 1.252 sec/batch)\n",
      "step 2490, loss=1.04 (92.7 example/sec; 1.382 sec/batch)\n",
      "step 2500, loss=1.06 (111.5 example/sec; 1.148 sec/batch)\n",
      "step 2510, loss=1.13 (110.4 example/sec; 1.159 sec/batch)\n",
      "step 2520, loss=0.90 (105.1 example/sec; 1.217 sec/batch)\n",
      "step 2530, loss=1.05 (116.0 example/sec; 1.104 sec/batch)\n",
      "step 2540, loss=1.02 (104.3 example/sec; 1.227 sec/batch)\n",
      "step 2550, loss=1.21 (111.7 example/sec; 1.146 sec/batch)\n",
      "step 2560, loss=1.24 (113.5 example/sec; 1.127 sec/batch)\n",
      "step 2570, loss=0.94 (112.0 example/sec; 1.143 sec/batch)\n",
      "step 2580, loss=0.92 (110.9 example/sec; 1.154 sec/batch)\n",
      "step 2590, loss=1.09 (110.2 example/sec; 1.162 sec/batch)\n",
      "step 2600, loss=1.06 (107.4 example/sec; 1.192 sec/batch)\n",
      "step 2610, loss=1.03 (99.9 example/sec; 1.282 sec/batch)\n",
      "step 2620, loss=1.01 (104.6 example/sec; 1.224 sec/batch)\n",
      "step 2630, loss=1.25 (101.3 example/sec; 1.263 sec/batch)\n",
      "step 2640, loss=1.16 (104.6 example/sec; 1.224 sec/batch)\n",
      "step 2650, loss=1.05 (90.2 example/sec; 1.418 sec/batch)\n",
      "step 2660, loss=0.87 (92.0 example/sec; 1.392 sec/batch)\n",
      "step 2670, loss=0.86 (94.2 example/sec; 1.358 sec/batch)\n",
      "step 2680, loss=1.15 (111.5 example/sec; 1.148 sec/batch)\n",
      "step 2690, loss=0.93 (94.0 example/sec; 1.361 sec/batch)\n",
      "step 2700, loss=0.98 (108.0 example/sec; 1.185 sec/batch)\n",
      "step 2710, loss=0.88 (97.0 example/sec; 1.319 sec/batch)\n",
      "step 2720, loss=1.08 (103.8 example/sec; 1.233 sec/batch)\n",
      "step 2730, loss=0.99 (98.7 example/sec; 1.297 sec/batch)\n",
      "step 2740, loss=1.12 (93.4 example/sec; 1.370 sec/batch)\n",
      "step 2750, loss=1.10 (99.3 example/sec; 1.289 sec/batch)\n",
      "step 2760, loss=1.06 (103.4 example/sec; 1.237 sec/batch)\n",
      "step 2770, loss=1.00 (101.9 example/sec; 1.256 sec/batch)\n",
      "step 2780, loss=1.00 (107.8 example/sec; 1.187 sec/batch)\n",
      "step 2790, loss=1.04 (111.4 example/sec; 1.149 sec/batch)\n",
      "step 2800, loss=0.99 (106.3 example/sec; 1.204 sec/batch)\n",
      "step 2810, loss=1.15 (107.4 example/sec; 1.192 sec/batch)\n",
      "step 2820, loss=1.07 (96.4 example/sec; 1.328 sec/batch)\n",
      "step 2830, loss=1.00 (95.9 example/sec; 1.334 sec/batch)\n",
      "step 2840, loss=1.25 (104.2 example/sec; 1.228 sec/batch)\n",
      "step 2850, loss=1.03 (109.4 example/sec; 1.170 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2860, loss=1.13 (101.4 example/sec; 1.262 sec/batch)\n",
      "step 2870, loss=1.19 (100.8 example/sec; 1.269 sec/batch)\n",
      "step 2880, loss=1.02 (102.5 example/sec; 1.248 sec/batch)\n",
      "step 2890, loss=1.11 (99.7 example/sec; 1.284 sec/batch)\n",
      "step 2900, loss=1.06 (101.1 example/sec; 1.266 sec/batch)\n",
      "step 2910, loss=1.00 (106.0 example/sec; 1.207 sec/batch)\n",
      "step 2920, loss=1.08 (109.2 example/sec; 1.172 sec/batch)\n",
      "step 2930, loss=1.10 (106.2 example/sec; 1.205 sec/batch)\n",
      "step 2940, loss=1.05 (97.2 example/sec; 1.317 sec/batch)\n",
      "step 2950, loss=1.13 (99.4 example/sec; 1.287 sec/batch)\n",
      "step 2960, loss=1.24 (114.0 example/sec; 1.123 sec/batch)\n",
      "step 2970, loss=1.10 (107.0 example/sec; 1.196 sec/batch)\n",
      "step 2980, loss=1.16 (80.9 example/sec; 1.583 sec/batch)\n",
      "step 2990, loss=1.09 (113.5 example/sec; 1.128 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_step):\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    _,loss_value = sess.run([train_op, loss],\n",
    "                           feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    if step % 10 == 0:\n",
    "        example_per_sec = batch_size/duration\n",
    "        sec_per_batch = float(duration)\n",
    "        format_str = ('step %d, loss=%.2f (%.1f example/sec; %.3f sec/batch)')\n",
    "        print(format_str %(step, loss_value, example_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 10000\n",
    "import math\n",
    "num_iter = int(math.ceil(num_examples/batch_size))\n",
    "ture_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "while step < num_iter:\n",
    "    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "    predictions = sess.run([top_k_op], feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
    "    ture_count += np.sum(predictions)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ 1 = 0.725\n"
     ]
    }
   ],
   "source": [
    "precision = ture_count / total_sample_count\n",
    "print('precision @ 1 = %.3f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，数据增强在训练中作用很大，它可以给单幅图增加多个副本，大大增加了样本量，从而提高图片的利用率，防止对某一张图片结构的学习过拟合。图片本身冗余信息比较多，因此，可以制造不同的噪声并让图片依然可以被识别出来。如果神经网络可以克服这些噪声，那么它的泛化性一定很好。下图是数据量和不同的模型准确率的关系。![img/1.png](img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面和之前的例子可以看到，卷积层一般需要和一个池化层链接，卷积加池化已经是图像识别的一个标准组件了。后面的全连接层的目的是输出分类结果，前面卷积层是特征提取工作，直到最后全连接层才开始对特征进行组合匹配并进行分类。\n",
    "\n",
    "卷积层的训练相比于全连接层更复杂，尽管全连接层参数占全部参数的90%多。训练全连接层基本是进行一些矩阵乘法运算，而卷积层的训练基本依赖于cuDNN的实现。算法相对复杂，有些方法还会涉及傅里叶变换。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
