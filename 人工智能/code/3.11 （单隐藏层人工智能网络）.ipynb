{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单隐藏层人工智能网络\n",
    "\n",
    "> 这个笔记是实现课程中的单隐藏层的人工智能网络,模型的示意图如下图所示.\n",
    "\n",
    "**单隐藏层人工智能神经网络**\n",
    "![单隐层神经网络示意图](img/one_hiden_layer.png)\n",
    "\n",
    "在上面的神经网络示意图中，在第一层的激活函数的使用上面，我准备试用tanh函数作为激活函数，在输出层函数上面我准备试用sigmoid函数作为激活函数。\n",
    "它们的函数如下所示：\n",
    "![sigmoid&tanh](img/sig_and_tanh.png)\n",
    "\n",
    "OK,下面开始我们的代码编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 训练的数据集\n",
    "train_input = np.array([[1, 2, 4], [4, 6, 8], [4, 5, 6], [3, 5, 9], [2, 4, 0], [9, 2, 7]])\n",
    "train_output = np.array([1,0,0, 1, 1, 0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    '''\n",
    "        sigmoid函数\n",
    "    '''\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_derivatives(Z):\n",
    "    '''\n",
    "        tanh函数的导数\n",
    "    '''\n",
    "    return 1 - np.tanh(Z)*np.tanh(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_demin(X, Y):\n",
    "    '''\n",
    "        获取输入参数的维度。分别是nx以及m\n",
    "    '''\n",
    "    X_dimen = np.shape(X)\n",
    "    nx = X_dimen[0]\n",
    "    m = X_dimen[1]\n",
    "    \n",
    "    Y_dimen = np.shape(Y)\n",
    "    # 检测维度是否正确\n",
    "    if m == Y_dimen[0]:\n",
    "        return nx, m\n",
    "    else:\n",
    "        print(\"Your Demin Size not match!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_value():\n",
    "    '''\n",
    "        初始化单隐层神经网络的参数\n",
    "    '''\n",
    "    # 初始化各个层的参数,这里应该注意到，我们实现的是上面提到的模型，因此模型各层的节点数就按照确定的数字写了\n",
    "    #  1.第一层参数W_1(4*3), B_1(4*1)\n",
    "    W_1 = np.random.randn(4, 3)*0.01\n",
    "    B_1 = np.zeros((4,1))\n",
    "    #  2.第二层的参数W_2(1,4), B_2(1*1)\n",
    "    W_2 = np.random.randn(1, 4)*0.01\n",
    "    B_2 = np.zeros((1,1))\n",
    "    return W_1, B_1, W_2, B_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate(X, W_1, B_1, W_2, B_2):\n",
    "    '''\n",
    "        计算单隐层神经网络的输出，输出结果包括\n",
    "        第一层: Z_1, A_1\n",
    "        第二层: Z_2, A_2\n",
    "    '''\n",
    "    # 1.计算第一层的输出,numpy中居然有tanh这个函数，太好了不用自己实现了\n",
    "    Z_1 = np.dot(W_1, X) + B_1\n",
    "    A_1 = np.tanh(Z_1)\n",
    "    # 2.计算第二层的输出\n",
    "    Z_2 = np.dot(W_2, A_1) + B_2\n",
    "    A_2 = sigmoid(Z_2)\n",
    "    return Z_1, A_1, Z_2, A_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_hiden_layer_neural_network(X, Y, a = 0.01):\n",
    "    '''\n",
    "        这个模型用来训练单隐层神经网络，如果您的神经网络的列数是训练集的个数，可以使用X.T来进行转置输入\n",
    "        X：作为训练神经网络的输入，其中X的行数应该是训练样本单个样本的维度，X的列数应该是训练集的个数。如果对应错误可能会导致训练错误。或者训练的结果和预期的结果不一致\n",
    "        Y：作为X的预期输出，因此这个网络神经模型是监督性学习\n",
    "        a：神经网络的学习率。用于调控神经网络的学习效率。\n",
    "    '''\n",
    "    # 获取输入参数的维度\n",
    "    nx,m = get_train_demin(X, Y)\n",
    "    \n",
    "    # 初始化各层参数\n",
    "    W_1, B_1, W_2, B_2 = init_value()\n",
    "    \n",
    "    for i in range(10000):\n",
    "        # 进行计算\n",
    "        Z_1, A_1, Z_2, A_2 = calculate(X, W_1, B_1, W_2, B_2)\n",
    "\n",
    "        # 计算导数\n",
    "        # 1.计算第二层的导数\n",
    "        dZ_2 = A_2 - Y\n",
    "        dW_2 = np.dot(dZ_2, A_1.T)/m\n",
    "        dB_2 = np.sum(dZ_2, axis = 1, keepdims = True)/m\n",
    "        # 2.计算第一层导数l\n",
    "        dZ_1 = np.dot(W_2.T, dZ_2) *  tanh_derivatives(Z_1)\n",
    "        dW_1 = np.dot(dZ_1, X.T)/m\n",
    "        dB_1 = np.sum(dZ_1, axis = 1, keepdims = True)/m\n",
    "\n",
    "        # 更新参数\n",
    "        W_1 -= a * dW_1\n",
    "        B_1 -= a * dB_1\n",
    "        W_2 -= a * dW_2\n",
    "        B_2 -= a * dB_2\n",
    "    # Over !!!\n",
    "    return W_1, B_1, W_2, B_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上面代码的简单解释\n",
    "上面只要的函数就是***train_one_hiden_layer_neural_network***这个函数，它实现了训练模型的目的。\n",
    "它的步骤如下：\n",
    "1. 初始化模型参数\n",
    "2. 进行正向传播计算，计算出各个层的输出\n",
    "3. 进行反向传播计算，算出各层各参数的导数\n",
    "4. 根据各参数的导数运用梯度下降法更新模型参数\n",
    "5. 重复上面的2-4的运算\n",
    "6. 将运算完成后的参数返回\n",
    "\n",
    "## 进行测试\n",
    "\n",
    "上面我们已经写完了我们的单隐层人工智能模型，下面进行测试，我们看一下训练完成之后的模型输出的各层的参数是什么。\n",
    "然后我们再来看一下我们再用同样的训练集进行测试，看一下正确率能达到多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Over The W_1 Value is :[[-1.40142554  0.58420185  0.08085084]\n",
      " [-1.82966071  0.76318787  0.11005912]\n",
      " [ 1.72588941 -0.71979153 -0.1030479 ]\n",
      " [ 0.88658064 -0.375753   -0.03827359]]\n",
      "Train Over The B_1 Value is :[[ 0.91757881]\n",
      " [ 1.20296599]\n",
      " [-1.13439402]\n",
      " [-0.56625279]]\n",
      "Train Over The W_2 Value is :[[ 1.97970072  2.65023211 -2.48118035 -1.25163754]]\n",
      "Train Over The B_2 Value is :[[ 0.91757881]\n",
      " [ 1.20296599]\n",
      " [-1.13439402]\n",
      " [-0.56625279]]\n"
     ]
    }
   ],
   "source": [
    "# 查看模型训练后的参数情况\n",
    "W_1, B_1, W_2, B_2 = train_one_hiden_layer_neural_network(train_input.T, train_output)\n",
    "print(\"Train Over The W_1 Value is :\" + str(W_1))\n",
    "print(\"Train Over The B_1 Value is :\" + str(B_1))\n",
    "print(\"Train Over The W_2 Value is :\" + str(W_2))\n",
    "print(\"Train Over The B_2 Value is :\" + str(B_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test result is:[[  9.99321147e-01   2.48479264e-02   1.35680371e-03   9.80712232e-01\n",
      "    9.89830837e-01   4.69859999e-04]]\n"
     ]
    }
   ],
   "source": [
    "# 进行测试\n",
    "_,_1,_2,A_2 = calculate(train_input.T, W_1, B_1, W_2, B_2)\n",
    "print(\"test result is:\" + str(A_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试结果\n",
    "我们用同一个数据集对模型训练了10000次，最终我们训练结束后的输出如上，我们看到我们的期望输出是：\n",
    "    \n",
    "    train_output = np.array([1,0,0, 1, 1, 0,])\n",
    "    \n",
    "最终的输出是：\n",
    "    \n",
    "    [  0.999321147   0.0248479264   0.00135680371   0.980712232    0.989830837   0.000469859999]]\n",
    "\n",
    "经过四舍五入之后，它的结果是：\n",
    "    [     1               0             0               1              1                0]\n",
    "\n",
    "很明显，正确率也达到了**100%**\n",
    "\n",
    "## 那么我们来看一下它的相对误差把"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均绝对误差为：0.00946839572252\n"
     ]
    }
   ],
   "source": [
    "relative_error = (np.sum(np.abs(A_2 - train_output)))/np.shape(train_output)[0] \n",
    "print(\"平均绝对误差为：\" + str(relative_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比[2.14（我的第一个logistic梯度回归模型）](2.14（我的第一个logistic梯度回归模型）.ipynb)可以看到\n",
    "\n",
    "在这两个模型中，我们都对同一个训练集 训练了10000次，最终得到结果都能拟合出一个很好的结果，当然，这里的数据我都是随意敲出来的随机的数据，他们之间的规律性我也不知道，因此也不能拿什么别的数据去进行测试，因此在这里我用了原有的训练模型的数据去进行测试，来查看最终的训练结果。\n",
    "\n",
    "另外，虽然两个模型都达到了很好的拟合效果，但是我们应该看到，使用隐藏层的神经网络模型的模型的平均绝对误差要更小，也就是他的效果要更好，但是，刚开始测试的时候，我只对上面的模型训练了1000次，得到的结果却不是那么理想，平均绝对误差居然在0.4-0.5之间，这是一个很大的误差！！于是我把训练次数增加到了10000次于是对于这两个模型，我总结如下：\n",
    "\n",
    "1. 如果相同训练集，logistic回归和单隐层神经网络在训练次数较少的情况下，logistic回归的效果要更好一些\n",
    "2. 如果数据集越大，训练时间越长，拥有隐藏层的神经网络要优于logistic回归\n",
    "\n",
    "其实想想也知道，有隐藏层的神经网络是建立在logistic回归上面的更复杂的系统，要想得到更好的效果，下面两个点是需要具备的：\n",
    "1. 更多的训练集和训练次数\n",
    "2. 更复杂的系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
