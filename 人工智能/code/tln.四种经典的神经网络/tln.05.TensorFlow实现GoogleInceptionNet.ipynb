{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleInceptionNet\n",
    "+ **作者**：Google\n",
    "+ **特点**：控制了计算量和参数量的同时，获得了非常好的分类性能——top5 error rate 6.67%\n",
    " Inception V1有22层深，比AlexNet的8层或者VGGNet的19层还要深，但其计算量只有15一次浮点运算，只有500万参数，仅为AlexNet的1/12\n",
    " Inception V1 参数少但效果好的原因除了模型层数更深，表达能力更强外，还有两点：\n",
    "  1. 去除了最后的全连接层，用全局平均池化层（即将图片尺寸变为1x1来取代，全连接层几乎占据了AlexNet或者VGGNet90%的参数量。去除全连接层后模型训练更快并且减轻了过拟合。用全局平均池化层取代全连接层的做法借鉴了Network In NetWork论文）。\n",
    "  2. Inception V1中精心设计的InceptionModule提高了参数的利用率。InceptionModule本身如同大网络中的一个小网络，其结构可以反复堆叠在一起形成大网络。InceptionV1比NIN增加了分支 网络，NIN主要是级联的卷积层和MLPConv层，卷积层必须通过增加输出通道数提升表达能力，但是会导致计算量增大以及过拟合，每一个输出通道对应一个滤波器，同一个滤波器共享参数，只能提取一类特征，因此一个输出通道智能做一种特征处理。NIN中的MLPConv层拥有更强大的能力，允许在输出通道之间组合信息。MLPConv基本等效于普通卷积层后再链接1x1卷积核ReLU激活函数。\n",
    "+ **结构**：\n",
    " ![InceptionModule](img/InceptionModule.png)\n",
    " + 可以看到，Inception Module有四个分支：\n",
    "   1. 对输入进行1x1卷积,这也是NIN中提出的一个重要结构,可以跨通道组织信息，提高网络表达能力。同时还可以对输出通道升降维。Inception Module四个分支都用到了1x1卷积，来进行降低成本的跨通道的特征变换。\n",
    "   2. 先使用1x1卷积，再连接3x3卷积，相当于进行了两次特征变换\n",
    "   3. 和第二个分支类似，先1x1，再5x5\n",
    "   4. 3x3最大池化后直接使用1x1卷积。\n",
    "   可以看到，上面四个分支都使用了1x1卷积，这是因为1x1卷积性价比很高，用很少的计算量就能增加一层特征变换和非线性化。\n",
    "   + 最后有一个聚合操作。InceptionModule包含了3中不同尺寸的卷积以及一个最大池化，增加了网络对不同尺度的适应性。这一部分和Multi-Scale的思想类似。\n",
    "   早期ComputerVersion研究中，受灵长类神经视觉系统的启发，Serre使用不同尺寸的Gabor滤波器处理不同尺寸的图片，InceptionV1借鉴了这种思想，InceptionV1的论文中指出，InceptionModule可以让网络深度和宽度高效率扩充，提高准确率且不至于过拟合。\n",
    " \n",
    " 人脑神经元链接是稀疏的。研究者认为大型神经网络的合理链接方式也应是稀疏的，可以减轻过拟合并降低计算量，例如卷积神经网络就是稀疏的链接。InceptionNet目标就是找到最优的稀疏结构单元。其稀疏结构基于**Hebbian原理**：神经反射活动的持续与重复会导致神经元链接稳定性持久提升，“一起发射的神经元会连在一起，学习过程中的刺激会使神经元件的突触强度增加。”\n",
    " \n",
    " *Provable Bounds for Learning Some Deepp Representations*提出，如果数据集的概率分布可以被一个很大很稀疏的神经网络所表达，那么构筑这个网络的最佳方式是逐层构建网络，将上一层高度相关的节点聚类，并将聚类出来的每一个小簇连接到一起。\n",
    " ![6.11](img/6.11.png)\n",
    " \n",
    " 因此，好的稀疏结构应该符合*Hebbian*原理的。应该把相关性高的一簇神经元节点连接在一起。图片数据中，临近区域数据相关性高，因此相邻的像素点被卷及操作连接在一起。我们可能有多个卷积核，在**同一空间位置但在不同通道的卷积核输出结果相关性极高**，因此，一个1x1的卷积就可以很自然地把这些相关性很高的，在同一个空间位置但是不同通道的特征连接在一起。这就是频繁应用1x1卷积核的原因。\n",
    " 同理，3x3,5x5卷积连接的节点相关性也很高，因此添加一些大尺寸卷积，增加多样性，最后InceptionModule将相关性很高的节点连接在一起，就完成了设计初衷，构建出了很高效的符合Hebbian原理的稀疏结构。\n",
    " \n",
    " InceptionModule中，通常1x1卷积的输出通道占比会比较高。3x3和5x5占比稍低，因为我们希望靠后的InceptionModule可以捕捉高阶特征，因此靠后的卷积空间集中度应该较低，这样可以捕获大面积特征，因此，越靠后的InceptionModule中，3x3和5x5的卷积输出占比应该越高。\n",
    " \n",
    " InceptionNet有22层深，除了最后一层的输出，中间节点分类效果也很好，因此在InceptionNet中，还是用了辅助分类节点，将中间某一层的输出用作分类，按一个较小的权重（0.3）加到最终分类结果中。相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供饿了额外的正则化。\n",
    "\n",
    "+ **成就**：ILSVRC2014比赛第一名\n",
    "\n",
    "+ **家族**\n",
    " 1. 2014.09 *Going Deeper with Convolutions* Inception V1(top5 error rate 6.67)\n",
    " 2. 2015.02 *Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate* Inception V2(top5 error rate 4.8)\n",
    " 3. 2015.12 *Rethining the Inception Architecture for Computer Vision* Inception V3 (top5 error rate 3.5)\n",
    " 4. 2016.02 *Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning* Inception V4 (top5 error rate 3.08)\n",
    " \n",
    "+ **InceptionV2**\n",
    " 学习了VGGNet，使用3x3堆叠卷积代替5x5卷积，提出了**BatchNormalization**，一个有效的正则化方法，可以让大型卷积网络训练速度加快很多倍，准确率也大幅提高，\n",
    " BN用于某层时，会对每一个mini-batch进行标准化处理，使输出规范到N(0,1)的正态分布，减少了InternalConvariateShift（内部神经元分布的改变）。\n",
    " \n",
    " BN论文指出，传统深度学习在训练时，每一层输出分布都在变化，导致训练变得困难，智能用较小的学习率解决。而使用BN后，可以有效解决这个问题，学习速率可以增大很多倍，达到之前的准确率所需迭代次数只有1/14,BN某种意义上起到了正则化作用，因此可以减少Dropout，简化网络结构。\n",
    " \n",
    "+ 使用BN要做的一些调整\n",
    "  - 增大学习率\n",
    "  - 去除Dropout\n",
    "  - 减轻L2正则\n",
    "  - 去除LRN\n",
    "  - 更彻底地对训练样本进行shuffle\n",
    "  - 减少数据增强过程对数据的光学畸变（BN训练更快，每个样本被训练次数更少，因此真实的数据更有帮助）\n",
    "  \n",
    "+ **InceptionV3**\n",
    "   + 引入Factorization into small convolutions思想，将较大二维卷积拆成两个较小的1维卷积。如将7x7卷积拆成1x7和7x1卷积。加速运算节约参数减轻过拟合，论文指出，这种非对称的卷积结构拆分，结果比对称拆为几个相同的卷积核效果更明显，可以处理更多，更丰富的空间特征，增加特征多样性。\n",
    "   ![3_3to1_3and1_3.png](img/3_3to1_3and1_3.png)\n",
    "   + 优化了InceptionModule结构，现在InceptionModule有35x35,17x17,8x8三种不同结构。这些InceptionModule只在网络后部出现。并且除了在InceptionModule中使用分支，还在分支中使用了分支（8x8结构中），可以说是Network in Network in Network。\n",
    "   ![InceptionV3Module.png](img/InceptionV3Module.png)\n",
    "\n",
    "+ **InceptionV4**\n",
    " 结合了微软的ResNet。\n",
    " \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面实现InceptionV3，其网络结构如下表所示\n",
    "![InceptionV3](img/InceptionV3.png)\n",
    "\n",
    "这里使用tf.contrib.slim辅助设计这个网络，只需要少量的代码即可构建好有42层深的InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先使用lambda语法定义一个简单的函数，用来产生截断的正态分布函数**trunc_normal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "trunc_normal = lambda stddev:tf.truncated_normal_initializer(0.0, stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义函数inception_v3_scope函数，用来生成网络中经常用到的默认的参数。比如卷积的激活函数，权重的初始化方式，标准化器等。\n",
    "+ L2正则的weight_decay默认为0.00004\n",
    "+ 标准差为0.1\n",
    "+ 参数batch_norm_var_collection默认值为moving_vars.\n",
    "\n",
    "接下来定义batch_normalization的参数字典。\n",
    "+ 衰减系数 decay : 0.9997\n",
    "+ epsilon : 0.001\n",
    "+ updates_collections : tf.GraphKeys.UPDATE_OPS\n",
    "+ variables_collections \n",
    "  - beta : None \n",
    "  - gamma : None\n",
    "  - moving_mean : batch_norm_var_collection\n",
    "  - moving_variance : batch_norm_var_collection\n",
    "  \n",
    "之后使用slim.arg_scope函数，这是一个非常有用的工具，它可以给函数的参数自动赋予某些之，例如下面这句：\n",
    "```python\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                 weights_regularizer=slim.l2_regularizer(weight_decay))\n",
    "```\n",
    "会对\\[slim.conv2d, slim.fully_connected\\]这两个函数自动赋值，将参数weights_regularizers的值默认设为slim.l2_regularizer(weight_decay),使用了slim.arg_scope就不用每次都重复设置参数了。只需要在有修改时设置。\n",
    "\n",
    "之后，嵌套一个slim.arg_scope,对卷积层生成函数slim.conv2d的几个参数赋予默认值\n",
    "+ 权重初始化器：weights_initializer ： trunc_normal\n",
    "+ 激活函数 ：activation_fn ： ReLU\n",
    "+ 标准化器 ：normalizer_fn ： batch_norm\n",
    "+ 标准化器的参数设置为前面定义的 batch_norm_params\n",
    "\n",
    "最后返回定义好的scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3_arg_scope(weight_decay=0.00004, stddev=0.1, batch_norm_var_collection='moving_vars'):\n",
    "    \n",
    "    # 定义batch_normalization 的参数字典\n",
    "    batch_norm_params = {\n",
    "        'decay':0.9997,\n",
    "        'epsilon':0.001,\n",
    "        'updates_collections':tf.GraphKeys.UPDATE_OPS,\n",
    "        'variables_collections':{\n",
    "            'beta':None,\n",
    "            'gamma':None,\n",
    "            'moving_mean':[batch_norm_var_collection],\n",
    "            'moving_variance':[batch_norm_var_collection],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 设置conv2d和fully_connected的weights_regularizer默认函数，为卷积层和全连接层的参数设置L2正则化\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "        # 设置卷积操作的权重初始化，激活函数，batch_norm函数，以及batch_norm的默认参数\n",
    "        with slim.arg_scope([slim.conv2d], \n",
    "                            weights_initializer=trunc_normal(stddev=stddev),\n",
    "                            activation_fn = tf.nn.relu,\n",
    "                            normalizer_fn = slim.batch_norm,\n",
    "                            normalizer_params = batch_norm_params) as sc:\n",
    "            return sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为上面实现定义好了slim.conv2d中的各种默认参数，因此下面在使用的时候就会比较简单。只需要一行代码就可以定义一个卷积层。\n",
    "\n",
    "接下来，我们定义inception_v3_base函数，可以用它来生成InceptionV3的网络的卷积部分，参数inputs是输入图片数据的Tensor，scope包含了函数默认参数的环境。定义一个字典end_points, 用来保存某些关键点用来之后使用。\n",
    "\n",
    "我们准备分步骤实现整个网络的框架，首先，我们先实现上面图表中的InceptionModule之前的那些卷积层的操作，我们定义一个函数来实现上面的操作，即上面图表中开始的六个卷积层和一个池化层\n",
    "![conv1](img/inceptionv3conv1.png)\n",
    "我们定义函数Inception_v3_conv_1\n",
    "\n",
    "首先使用slim.arg_scope对slim.conv2d,slim.max_pool2d,slim.avg_pool2d进行设置默认参数，\n",
    "+ 这里使用slim.conv2d创建卷积层，它的参数如下:\n",
    "    1. inputs 输入的Tensor\n",
    "    2. 输出通道数\n",
    "    3. 卷积核尺寸\n",
    "    4. stride 步长\n",
    "    5. padding模式 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3_conv_1(inputs,scope=None):\n",
    "    with slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d], stride=1,padding=\"VALID\", scope=scope):\n",
    "        net = slim.conv2d(inputs, 32, [3, 3], stride=2, scope=\"Conv2d_1a_3x3\")\n",
    "        net = slim.conv2d(net, 32, [3, 3], scope=\"Conv2d_2a_3x3\")\n",
    "        net = slim.conv2d(net, 64, [3, 3], padding='SAME', scope=\"Conv2d_2b_3x3\")\n",
    "        net = slim.max_pool2d(net, [3, 3], stride=2, scope=\"MaxPool_3a_3x3\")\n",
    "        net = slim.conv2d(net, 80, [1, 1], scope=\"Conv2d_3b_1x1\")\n",
    "        net = slim.conv2d(net, 192, [3, 3], scope=\"Conv2d_4a_3x3\")\n",
    "        net = slim.max_pool2d(net, [3, 3], stride=2, scope=\"MaxPool_5a_3x3\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面面代码我们可以观察到，在前面几个非InceptionModule的卷积层中，主要使用了3x3的小卷积核，这是充分借鉴了VGGNet的结构。同时InceptionV3论文总也提出了Factorization into small convolutions 思想，利用连个1维卷积模拟大尺寸的2维卷积，减少参数量的同时增加非线性。前面几层卷积层中还有1x1卷积，可低成本跨通道对特征进行组合。\n",
    "另外，卷积层除了第一个的步长是2，其余的都是1，而池化层尺寸是3x3，步长为2的重叠最大池化，这是AlexNet中用过的结构。\n",
    "\n",
    "网络的输入尺寸为299x299x3,在经历3个步长为2的层之后，尺寸缩小为35x35x192,空间尺寸大大降低，但是输出通道增加了很多。这部分代码共有4个卷积层，2个池化层，实现了对输入图片数据的尺寸压缩，并对图片特征进行了抽象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是三个InceptionModule组，包含了3个结构类似的InceptionModule，他们结构和上面的6-13的第一幅图有点类似。\n",
    "![modules1](img/modules1.png)\n",
    "\n",
    "#### 下面实现第一个模块组，它包含有三个InceptionModule\n",
    "\n",
    "我们先实现**第一个模块组的第一个InceptionModule**，名称为*Mixed_5b*,同上，我们先使用slim.arg_scope设置所有InceptionModule模块组的参数，所有层步长设为1，padding设为SAME。然后设置这个InceptionModule的variable_scope为Mixed_5b.\n",
    "函数名称设置为inception_module_1_1\n",
    "\n",
    "这个InceptionModule有四个分支，从branch_0到branch_3.\n",
    "1. 第一个分支 64个输出通道的1x1卷积\n",
    "2. 第二个分支 48个输出通道的1x1卷积，链接64个输出通道的5x5卷积\n",
    "3. 第三个分支 64个输出通道的1x1卷积，连接两个96输出通道的3x3卷积\n",
    "4. 第四个分支 3x3的平均池化，链接32个输出通道的1x1卷积\n",
    "\n",
    "最后，使用tf.concat将四个分支的输出合并到一起（在第3个维度合并)，即输出通道上面合并。因为这里所有层的步长都为1，且padding模式为SAME，因此尺寸不会缩小，依然会维持在35x35,最终输出通道数之和为64+64+96+32=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module1_1(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_5b\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "                branch_0 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 48, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope=\"Conv2d_0b_5x5\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0c_3x3\")\n",
    "        with tf.variable_scope(\"Branch_3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**之后实现第一个模块组的第二个InceptionModule（*Mixed_5c*）**。这一个module同样有4个分支，唯一和第一个Module不同的是，第四个分支接的是64输出通道的1x1卷积，之前是32个输出通道，因此，第二个module最终输出尺寸是35x35x288,通道数比之前增加了32个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module1_2(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_5c\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 48, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope=\"Conv2d_0b_5x5\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0c_3x3\")\n",
    "        with tf.variable_scope(\"Branch_3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一个模块组的第三个InceptionModule（*Mixed_5d*）,和上面第二个Module完全一样**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module1_3(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_5d\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 48, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope=\"Conv2d_0b_5x5\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\"Conv2d_0c_3x3\")\n",
    "        with tf.variable_scope(\"Branch_3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，上面的第一个模块组我们已经定义完了\n",
    "\n",
    "#### 接下来定义第二个模块组，它总共有5个InceptionModule\n",
    "其中第二个到第五个InceptionModule非常类似，它的结构如上面的图6-13的第二幅图所示。\n",
    "![modules2.png](img/modules2.png)\n",
    "**第二个模块组的第一个InceptionModule名称为Mixed_6a,它包含有3个分支**\n",
    "+ 第一个分支是一个384输出通道的3x3卷积,这个分支通道数超过了之前的通道数之和，不过步长为2，因此尺寸会被压缩，且Padding模式为VALID，因此图片尺寸缩小为17x17\n",
    "+ 第二个分支是有三层，分别是64输出通道的1x1卷积，两个96输出通道的3x3卷积，最后一层步长为2，Padding为VALID\n",
    "+ 第三个分支是3x3最大池化层，步长为2，padding为VALID\n",
    "最后使用tf.concat在输出通道上合并，最终的输出通道数为384+96+288=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module2_1(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_6a\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\"VALID\", scope=\"Conv2d_1a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "            branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2, padding=\"VALID\", scope=\"Conv2d_1a_1x1\")\n",
    "        with tf.variable_scope(\"Brabch_2\"):\n",
    "            branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\"VALID\", scope=\"MaxPool_1a_3x3\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二个模块组的第二个InceptionModule，有四个分支**\n",
    "+ 第一个分支是简单的192输出通道的1x1卷积\n",
    "+ 第二个分支有三个卷积层，分别是（128输出通道1x1）（128输出通道1x7）（192输出通道7x1）\n",
    "+ 第三个分支有五个卷积层，（128-1x1）（128-7x1）（128-1x7）（128-7x1）（192-1x7）\n",
    "+ 第四个分支是3x3平均池化层，链接192-1x1\n",
    "最后合并，输出通道个数为192+192+192+192=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module2_2(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_6b\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 128, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 128, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "            branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 128, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope=\"Conv2d_0b_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 128, [1, 7], scope=\"Conv2d_0c_1x7\")\n",
    "            branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope=\"Conv2d_0d_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope=\"Conv2d_0e_1x7\")\n",
    "        with tf.variable_scope(\"Branch_3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二个模块组的第三个InceptionModule**\n",
    "\n",
    "和第二个InceptionModule非常相似，只有一个地方不同，第二个分支和第三个分支的前几个卷积层的输出通道不同，从128变成了160,最终输出通道不变\n",
    "\n",
    "需要注意的是，网络每经过一个InceptionModule，即使Tensor尺寸不变，但是特征都相当于被重新精炼了一遍，其中丰富的卷积和非线性化对提升网络性能帮助很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module2_3(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_6c\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 160, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "            branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 160, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope=\"Conv2d_0b_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope=\"Conv2d_0c_1x7\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope=\"Conv2d_0d_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope=\"Conv2d_0e_1x7\")\n",
    "        with tf.variable_scope(\"Branch_3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二个模块组的第四个InceptionModule**\n",
    "\n",
    "这个InceptionModule和前一个InceptionModule一模一样，目的是通过InceptionModule精心设计的结构增加卷积和非线性，提炼特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module2_4(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_6d\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 160, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "            branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 160, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope=\"Conv2d_0b_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope=\"Conv2d_0c_1x7\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope=\"Conv2d_0d_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope=\"Conv2d_0e_1x7\")\n",
    "        with tf.variable_scope(\"Branch_3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二个模块组的第五个InceptionModule**\n",
    "\n",
    "这个InceptionModule依然和前面的InceptionModule一模一样，这是第二个模块组的最后一个InceptionModule，因此我们将这个输出存储到end_points中，作为AuxAuxiliary ClassClassifier辅助模型的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module2_5(inputs, end_points, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_6e\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 160, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "            branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 160, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope=\"Conv2d_0b_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope=\"Conv2d_0c_1x7\")\n",
    "            branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope=\"Conv2d_0d_7x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope=\"Conv2d_0e_1x7\")\n",
    "        with tf.variable_scope(\"Branch_3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "    end_points['Mixed_6e'] = net\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最后定义第三个模块组\n",
    "第三个模块组包含3个InceptionModule，其后两个InceptionModule结构非常类似，他们结构如6-13的第三幅图所示。\n",
    "![module3](img/module3.png)\n",
    "\n",
    "**第三个模块组的第一个InceptionModule**有三个分支\n",
    "+ 第一个分支 （192-1x1）（320-3x3步长为2，padding为VALID）\n",
    "+ 第二个分支 （192-1x1）（192-1x7）（192-7x1）（192-3x3步长2,padding为VALID）\n",
    "+ 第三个分支 （3x3最大池化层，步长2， padding为VALID）\n",
    "\n",
    "最终的输出通道数为320+192+768=1280,输出尺寸下降到了8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module3_1(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_7a\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_0 = slim.conv2d(branch_0, 320, [3, 3], stride=2, padding=\"VALID\", scope=\"Conv2d_1a_3x3\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope=\"Conv2d_0b_1x7\")\n",
    "            branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope=\"Conv2d_0c_7x1\")\n",
    "            branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2, padding=\"VALID\",scope=\"Conv2d_1a_3x3\")\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\"VALID\", scope=\"MaxPool_1a_3x3\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三个模块组的第二个InceptionModule**有四个分支\n",
    "+ 第一个分支 （320-1x1）\n",
    "+ 第二个分支 （384-1x1）（分成两个分支（384-1x3）（384-3x1）合并）\n",
    "+ 第三个分支 （448-1x1）（384-3x3）（拆成两个分支（384-1x3）（384-3x1）合并）\n",
    "+ 第四个分支 （3x3平均池化层）（192-1x1）\n",
    "\n",
    "合并，最终输出通道数为 320+（384+384）+（384+384）+192=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module3_2(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_7b\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 320, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = tf.concat([\n",
    "                slim.conv2d(branch_1, 384, [1, 3], scope=\"Conv2d_0b_1x3\"),\n",
    "                slim.conv2d(branch_1, 384, [3, 1], scope=\"Conv2d_0b_3x1\")\n",
    "            ], 3)\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 448, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 384, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "            branch_2 = tf.concat([\n",
    "                slim.conv2d(branch_2, 384, [1, 3], scope=\"Conv2d_0c_1x3\"),\n",
    "                slim.conv2d(branch_2, 384, [3, 1], scope=\"Conv2d_0d_3x1\")\n",
    "            ], 3)\n",
    "        with tf.variable_scope(\"Branch3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三个模块组最后一个InceptionModule和前面的InceptionModule一模一样**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module3_3(inputs, scope=None):\n",
    "    with tf.variable_scope(\"Mixed_7c\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(inputs, 320, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_1 = tf.concat([\n",
    "                slim.conv2d(branch_1, 384, [1, 3], scope=\"Conv2d_0b_1x3\"),\n",
    "                slim.conv2d(branch_1, 384, [3, 1], scope=\"Conv2d_0b_3x1\")\n",
    "            ], 3)\n",
    "        with tf.variable_scope(\"Branch_2\"):\n",
    "            branch_2 = slim.conv2d(inputs, 448, [1, 1], scope=\"Conv2d_0a_1x1\")\n",
    "            branch_2 = slim.conv2d(branch_2, 384, [3, 3], scope=\"Conv2d_0b_3x3\")\n",
    "            branch_2 = tf.concat([\n",
    "                slim.conv2d(branch_2, 384, [1, 3], scope=\"Conv2d_0c_1x3\"),\n",
    "                slim.conv2d(branch_2, 384, [3, 1], scope=\"Conv2d_0d_3x1\")\n",
    "            ], 3)\n",
    "        with tf.variable_scope(\"Branch3\"):\n",
    "            branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\"AvgPool_0a_3x3\")\n",
    "            branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope=\"Conv2d_0b_1x1\")\n",
    "    return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将上面定义好的各个模块的函数组合到一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3_base(inputs, scope=None):\n",
    "    end_points = {}\n",
    "    with tf.variable_scope(scope, \"InceptionV3\", [inputs]):\n",
    "        \n",
    "        # 非InceptionModule处理\n",
    "        net = inception_v3_conv_1(inputs)\n",
    "        \n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],stride=1, padding=\"SAME\"):\n",
    "            \n",
    "            # 第一个模块组的处理\n",
    "            net = inception_module1_1(net)\n",
    "            net = inception_module1_2(net)\n",
    "            net = inception_module1_3(net)\n",
    "            \n",
    "            # 第二个模块组的处理\n",
    "            net = inception_module2_1(net)\n",
    "            net = inception_module2_2(net)\n",
    "            net = inception_module2_3(net)\n",
    "            net = inception_module2_4(net)\n",
    "            net = inception_module2_5(net, end_points)\n",
    "            \n",
    "            # 第三个模块组的处理\n",
    "            net = inception_module3_1(net)\n",
    "            net = inception_module3_2(net)\n",
    "            net = inception_module3_3(net)\n",
    "        return net, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，InceptionV3网络的核心部分（卷积层部分）就完成了。上面的结构中，首先是5个卷积层和2个池化层交替的普通结构，然后是3个InceptionModule模块组。\n",
    "\n",
    "设计InceptionNet的一个重要原则是，图片尺寸是不断缩小的，从299x299经过5个步长为2的卷积层或池化层后，缩小为8x8，同时输出通道不断增加，从一开始的3到最后的2048。\n",
    "\n",
    "从这里可以看出，每一层卷积，池化或者Inception模块组目的都是将空间结构简化，同时将空间信息转化为高阶抽象的特征信息。即将空间的维度转换为通道维度。\n",
    "\n",
    "**可以发现，InceptionModule一般有四个分支：**\n",
    "1. 第一个分支一般是1x1卷积（简单抽象特征）\n",
    "2. 第二个分支一般是1x1卷积再接分解后的1xn和nx1卷积（复杂特征抽象）\n",
    "3. 第三个分支和第二个分支类似但是一般更深一些（复杂特征抽象）\n",
    "4. 第四个分支一般具有最大池化或平均池化（简化结构池化层）\n",
    "\n",
    "四种不同程度的特征抽象和变换有选择的保留不同层次的高阶特征，这样可以最大程度的丰富网络的表达能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来。实现InceptionV3的最后一个部分（全局平均池化，Softmax和Auxiliary Losits）\n",
    "\n",
    "我们定义一个函数来实现上面的功能，函数的参数有：\n",
    "1. **inputs** 上面卷积层的输出\n",
    "2. **num_classes** 最后需要分类的数量,这里1000是ILSVRC比赛数据集的种类\n",
    "3. **is_training** 标志是否是训练过程,对BatchNormalization和Dropout有影响\n",
    "4. **dropout_keep_prob** Dropout的保留比例,默认是0.8\n",
    "5. **prediction_fn** 最终用来分类的函数\n",
    "6. **spatial_squeeze** 是否对输出进行squueeze操作（去除维数为1的维度，比如将5x3x1转为5x3）\n",
    "7. **reuse** 是否会对网络和Variable进行重复使用\n",
    "8. **scope** 包含了函数默认参数的环境\n",
    "\n",
    "首先，使用tf.variable_scope定义网络的name和reuse等参数的默认值，然后使用前面定义好的inception_v3_base构筑整个网络的卷积部分，拿到最后一层的输出net和重要节点的字典表end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后是Auxiliary Logits这部分逻辑，**Auxiliary Logits最为辅助分类的节点**，对分类结果预测有很大帮助。\n",
    "+ 首先使用slim.arg_scope将卷积，最大池化，平均池化的步长设置为1，padding设置为SAME\n",
    "+ 然后通过end_points取得Mixed_6e\n",
    "+ 对取得的节点进行5x5池化，步长为3，padding为VALID\n",
    "+ 再进行128-1x1卷积核768-5x5卷积，padding为VALID。输出尺寸变为1x1x768\n",
    "+ 最后连接一个输出通道为num_classes的1x1卷积，这里不设置激活函数以及batch_norm的函数，权重初始化为标准差为0.001的正态分布\n",
    "+ 之后使用tf.suqeeze消除前两个为1的维度，最终将辅助节点添加到end_points中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auxiliary_logits(end_points, num_classes, scope=None):\n",
    "    # 获取上面卷积层中用于辅助分类的节点\n",
    "    aux_logits = end_points['Mixed_6e']\n",
    "    # 对辅助分类节点进行处理，使它的输出符合[batch_size,num_classes]的这种形式\n",
    "    # 也就是说通过卷积，池化操作将数据的weight和height维度变为1\n",
    "    with tf.variable_scope(\"AuxLogits\"):\n",
    "        aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3, padding=\"VALID\", scope=\"AvgPool_1a_5x5\")\n",
    "        aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope=\"Conv2d_1b_1x1\")\n",
    "        aux_logits = slim.conv2d(aux_logits, 768, [5, 5], weights_initializer=trunc_normal(0.01),padding=\"VALID\", scope=\"Conv2d_2a_5x5\")\n",
    "        aux_logits = slim.conv2d(aux_logits, num_classes, [1, 1], activation_fn=None, normalizer_fn=None,\n",
    "                                 weights_initializer=trunc_normal(0.001),scope=\"Conv2d_2b_1x1\")\n",
    "    return aux_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是正常的分类预测的逻辑了\n",
    "+ 首先直接对最后一层进行8x8的平均池化，padding设为VALID。这样，输出就变成了1x1x2048\n",
    "+ 再连接一个dropout层，节点的保留率设置为inceptionV3函数的参数dropout_keep_prob\n",
    "+ 在连接一个输出通道为num_classes的1x1卷积激活函数以及batch_norm都设置为None\n",
    "+ 同理，再使用tf.squeeze去除维度为1的维度\n",
    "+ 再连接一个softmax进行预测，返回包含输出结果logits以及包含辅助节点的end_points\n",
    "+ 最后，我们打印一些输出的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(net, end_points, dropout_keep_prob, num_classes, spatial_squeeze, prediction_fn, scope=None):    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        net = slim.avg_pool2d(net, [8, 8], padding=\"VALID\", scope=\"AvgPool_1a_8x8\")\n",
    "        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\"Dropout_1b\")\n",
    "        end_points[\"PreLogits\"] = net\n",
    "        logits = slim.conv2d(net, num_classes, [1, 1], \n",
    "                            activation_fn=None, normalizer_fn=None,scope=\"Conv2d_1c_1x1\")\n",
    "        if spatial_squeeze:\n",
    "            logits = tf.squeeze(logits, [1, 2], name=\"SpatialSqueeze\")\n",
    "        end_points[\"Logits\"] = logits\n",
    "        end_points[\"Predictions\"] = prediction_fn(logits, scope=\"Predictions\")\n",
    "        print(logits.op.name, \":\", logits.get_shape().as_list())\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3(inputs, num_classes=1000, is_training=True, dropout_keep_prob=0.8, \n",
    "                 prediction_fn=slim.softmax, spatial_squeeze=True, reuse=None, scope=\"InceptionV3\"):\n",
    "    \n",
    "    with tf.variable_scope(scope, \"InceptionV3\", [inputs, num_classes], reuse=reuse) as scope:\n",
    "        \n",
    "        # 设置batch_norm和dropout是否是训练，然后获取卷积层操作\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n",
    "            net, end_points = inception_v3_base(inputs, scope=scope)\n",
    "        \n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],stride=1,padding=\"SAME\"):\n",
    "                aux_logits = get_auxiliary_logits(end_points, num_classes)\n",
    "\n",
    "                if spatial_squeeze:\n",
    "                    # 因为最终的输出维度是[batch_size, weight, height, channel]\n",
    "                    # 在上面变换完成之后，输出维度是[batch_size, 1, 1, num_classes]\n",
    "                    # 因此，我们要把上面的weight和height去除掉\n",
    "                    # 因此最终会变成[batch_size, num_classes]\n",
    "                    aux_logits = tf.squeeze(aux_logits, [1, 2], name=\"SpatialSqueeze\")\n",
    "                    # 将处理完的辅助分类节点添加到end_points里面\n",
    "                end_points[\"AuxLogits\"] = aux_logits\n",
    "                \n",
    "                logits = get_logits(net, end_points, dropout_keep_prob, num_classes, spatial_squeeze, prediction_fn)\n",
    "                \n",
    "        return logits, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3/Logits/SpatialSqueeze : [32, 1000]\n"
     ]
    }
   ],
   "source": [
    "logfile = r\"D:\\python\\tensorflow\\googleInceptionNet\"\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.random_normal((32, 299, 299, 3))\n",
    "    net, end_points = inception_v3(inputs, 1000)\n",
    "# 创建写入数据的对象，设定log的文件夹，以及要导出的文件的graph数据流图对象\n",
    "writer = tf.summary.FileWriter(logdir=logfile, graph=graph)\n",
    "# 写入完成之后要关闭writer对象\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，上面整个InceptionNetV3就设计完了，InceptionV3是一个非常复杂巧妙的模型，用到了很多之前积累的设计大型深度学习的经验和技巧，虽然InceptionV3论文中给出了设计卷积神经网络的几个选择，但是其中很多超参数的选择包括层数，卷积核尺寸，池化的位置，步长的大小，factorization使用的时机，以及分支的设计，都很难一一解释。目前我们只能认为深度学习，尤其是大型卷积神经网络的设计，是一门实验学科，需要大量的探索和实践，很难证明某种神经网络一定更好，更多的是实验积累下来的经验总结出一些结论。深度学习研究中，理论证明部分依然是短板，但通过实验得到的结论通常也具有不错的推广性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_tensorflow_run(session, target, info_string):\n",
    "    print(\"start time is :%s\" %(datetime.now()))\n",
    "    durations = []\n",
    "    steps = []\n",
    "    num_steps_burn_in = 10\n",
    "    total_duration = 0.\n",
    "    total_duration_squared = 0.\n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(target)\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_steps_burn_in:\n",
    "            #print('%s: step %d, duration = %.3f' % (datetime.now(), i-num_steps_burn_in, duration))\n",
    "            durations.append(duration)\n",
    "            steps.append(i - num_steps_burn_in)\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "    mn = total_duration / num_batches\n",
    "    vr = total_duration_squared / num_batches - mn * mn\n",
    "    sd = math.sqrt(vr)\n",
    "    plt.plot(steps, durations)\n",
    "    plt.xlabel(\"setp\")\n",
    "    plt.ylabel(\"duration/sec\")\n",
    "    plt.title(info_string)\n",
    "    plt.show()\n",
    "    print(\"%s:%s across %d steps, %.3f +/- %.3f sec / batch\" %(datetime.now(), info_string, num_batches, mn, sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionV3/Logits/SpatialSqueeze : [32, 1000]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "height, width = 299, 299\n",
    "inputs = tf.random_uniform((batch_size, height, width, 3))\n",
    "with slim.arg_scope(inception_v3_arg_scope()):\n",
    "    logits, end_points = inception_v3(inputs, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time is :2018-02-21 11:42:24.262980\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a5ec97d978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-21 12:27:03.765730:Forward across 100 steps, 24.269 +/- 1.218 sec / batch\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "num_batches = 100\n",
    "time_tensorflow_run(sess, logits, \"Forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "InceptionNet中的一些经验和思想值得借鉴：\n",
    "+ **Factorization into small convolutions**\n",
    "+ **卷积神经网络从输入到输出，应让图片尺寸逐渐缩小，输出通道逐渐增加**\n",
    "+ **多分支提取不同程度的空间特征**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
